{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine_Tuning-Word_Truncated.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6ag0SQiLgJz"
      },
      "source": [
        "# Fine Tuning em Hiperparâmetros\n",
        "\n",
        "Os hiperparâmetros são valores que são definidos pelo(s) criador(es) do modelo para que ele funcione da forma esperada. Não há um valor pré-definido que irá funcionar perfeitamente para um modelo qualquer, então é preciso ajustar um valor que obterá uma performance melhor para o modelo. Esse processo é conhecido como fine tuning.\n",
        "\n",
        "O processo depende que alguns valores sejam testados no modelo e que a performance do modelo seja avaliada constantemente até que um valor razoável seja encontrado para o hiperparâmetro.\n",
        "\n",
        "Para esse modelo, os hiperparâmetros que passarão pelo processo de fine tuning são:\n",
        "\n",
        "- número de épocas (epoch)\n",
        "- taxa de aprendizado (learning rate)\n",
        "- taxa de decaimento (weight decay)\n",
        "\n",
        "## Fine tuning para 'Epoch'\n",
        "\n",
        "O processo de fine tuning para o hiperparâmetro de epoch será realizado de forma isolada. Isso significa que valores para os outros hiperparâmetros permanecerão fixos, enquanto o modelo é treinado para um número 'n' de epochs. Por meio dos passos de validação, durante o treinamento do modelo, é possível verificar a performance do modelo enquanto ele é treinado ao longo das épocas.\n",
        "\n",
        "Configuração:\n",
        "\n",
        "```\n",
        "    'BATCH_SIZE': 4,\n",
        "    'MAX_NUMBER_TOKENS': 512,\n",
        "    'NUMBER_OF_BRANCHES': 13,\n",
        "    'EPOCHS': 10,\n",
        "    'LEARNING_RATE': 2e-5,\n",
        "    'WEIGHT_DECAY': 0.01,\n",
        "    'WARM_UP_PROPORTION': 0.1\n",
        "```\n",
        "\n",
        "## Fine tuning para 'Taxa de Aprendizado' e 'Taxa de Decaimento'\n",
        "\n",
        "Para definir os valores dos hiperparâmetros 'taxa de aprendizado' e 'taxa de aquecimento', alguns valores foram escolhidos, de acordo com as sugestões [desse paper](https://arxiv.org/pdf/1905.05583.pdf). Dessa forma, todas as combinações de cada um dos valores que cada hiperparâmetro pode assumir será testado:\n",
        "\n",
        "- Taxa de aprendizado: 2.5e-5, 2e-5\n",
        "- Taxa de decaimento: 0.001, 0.01\n",
        "- Warm up proportion: 0.1, 0.3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ElRIFHVRpZS"
      },
      "source": [
        "## Inicialização e definiçao de constantes\n",
        "\n",
        "Como uma etapa inicial, toda a inicialização do notebook será concentrada no início desse documento. Os conteúdos contidos aqui são:\n",
        "\n",
        "1. Instalação de bibliotecas externas\n",
        "2. Importação de biblioteca\n",
        "3. Definição de valores constantes que podem ter seu uso replicado ao longo do notebook\n",
        "4. Inicialização do sistema de arquivos integrado ao Google Drive\n",
        "\n",
        "Ao fim dessa seção, os hiperparâmetros são definidos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCnd31PbV87o",
        "outputId": "6a3791cf-b130-48c6-a931-9e69cf328efa"
      },
      "source": [
        "# Installation of 3rd party libraries\n",
        "\n",
        "!pip install transformers\n",
        "!pip install --upgrade pytorch-lightning"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.10.2-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 12.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.16-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 45.6 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 32.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 35.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.16 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.2\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.4.6-py3-none-any.whl (922 kB)\n",
            "\u001b[K     |████████████████████████████████| 922 kB 12.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.19.5)\n",
            "Collecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Collecting torchmetrics>=0.4.0\n",
            "  Downloading torchmetrics-0.5.1-py3-none-any.whl (282 kB)\n",
            "\u001b[K     |████████████████████████████████| 282 kB 53.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.6.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.62.0)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 42.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (3.7.4.3)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2021.8.1-py3-none-any.whl (119 kB)\n",
            "\u001b[K     |████████████████████████████████| 119 kB 49.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (5.4.1)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.9.0+cu102)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 55.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (2.4.7)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.12.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (57.4.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.34.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.17.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.39.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.6.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.1)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 54.2 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 38.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.2.0)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.5.0)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=f2c3c8b92385527028124e32109104a2782fadc9f92d094c2591f48ab29d107b\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: multidict, yarl, async-timeout, fsspec, aiohttp, torchmetrics, pyDeprecate, future, pytorch-lightning\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 fsspec-2021.8.1 future-0.18.2 multidict-5.1.0 pyDeprecate-0.3.1 pytorch-lightning-1.4.6 torchmetrics-0.5.1 yarl-1.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK6fOkDJRmHV"
      },
      "source": [
        "# Imports\n",
        "\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import BertTokenizerFast as BertTokenizer, BertForSequenceClassification, AdamW, get_cosine_schedule_with_warmup, Trainer, TrainingArguments\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import re\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import seed_everything\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZCMObHNSEZd"
      },
      "source": [
        "# Constants\n",
        "\n",
        "CONSTANTS = {\n",
        "    'TRAINING_DATASET': '/content/drive/My Drive/MAC499 - Kaique e Yurick/DB/Train_Dataset.csv',\n",
        "    'VALIDATION_DATASET': '/content/drive/My Drive/MAC499 - Kaique e Yurick/DB/Validation_Dataset.csv',\n",
        "    'BERT_MODEL_NAME': 'neuralmind/bert-large-portuguese-cased',\n",
        "    'SEED': 13\n",
        "}\n",
        "\n",
        "# Hyperparameters\n",
        "\n",
        "HYPERPARAMETERS = {\n",
        "    'BATCH_SIZE': 2,\n",
        "    'MAX_NUMBER_TOKENS': 512,\n",
        "    'NUMBER_OF_BRANCHES': 13,\n",
        "    'EPOCHS': 3,\n",
        "    'LEARNING_RATE': 2.5e-5,\n",
        "    'WEIGHT_DECAY': 0.001,\n",
        "    'WARM_UP_PROPORTION': 0.1\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0nuV6t_SHNq"
      },
      "source": [
        "# Mounting Google Drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awJebaeaUWaK"
      },
      "source": [
        "## Verificar disponibilidade da GPU\n",
        "\n",
        "O próximo passo seria verificar se a GPU oferida pela Google gratuitamente como ambiente de execução do notebook está funcionando corretamente. A GPU oferece uma performance computacional maior em relação a calculos sendo executados pela CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R60zbZC_VrBo"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWjFOS75OGLK"
      },
      "source": [
        "### Reproducibilidade\n",
        "\n",
        "Para fins de reproducibilidade, definimos uma semente para o pytorch lightning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLZwg5GcOoTk"
      },
      "source": [
        "seed_everything(CONSTANTS['SEED'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtLPNEGuVtrX"
      },
      "source": [
        "## Carregar os dados\n",
        "\n",
        "Com os arquivos em mãos, é possível carregá-los para que os dados contidos possam ser utilizados para a criação do modelo.\n",
        "\n",
        "Nessa etapa, os dados são carregados a partir de arquivos .csv contendo as informações dos acórdãos. Esses arquivos .csv já foram pre-processados no notebook Data_Preprocessing.ipynb, que se encontra na pasta Projeto do Google Drive. No pre-processamento as classificações dos ramos do direito de cada acórdão são mapeadas para valores numéricos que o BERT consiga entender. Esse mapeamento segue o seguinte conjunto de chaves e valores:\n",
        "- Direito Penal (Direito Processual Penal) &rarr; 0\n",
        "- Direito Administrativo (Licitações, Contratos Administrativos, Servidores, Desapropriação, Tribunal de Contas, Improbidade, etc.) &rarr; 1\n",
        "- Direito Tributário/Direito Financeiro &rarr; 2\n",
        "- Direito Civil (Direito Comercial/Direito de Família) &rarr; 3\n",
        "- Direito Previdenciário &rarr; 4\n",
        "- Direito do Trabalho &rarr; 5\n",
        "- Direito Processual Civil &rarr; 6\n",
        "- Direito Eleitoral &rarr; 7\n",
        "- Direito do Consumidor &rarr; 8\n",
        "- Direito Internacional (Público ou Privado) &rarr; 9\n",
        "- Direito Militar &rarr; 10\n",
        "- Direito Econômico (Direito concorrencial e Agências Reguladoras Setoriais, Intervenção no Domínio Econômico) &rarr; 11\n",
        "- Direito Ambiental &rarr; 12\n",
        "\n",
        "Há dois conjunto de dados a serem carregados: treinamento e validação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIu2DL_KZ1G7"
      },
      "source": [
        "# Read the training dataset from .csv file\n",
        "documents = pd.read_csv(CONSTANTS['TRAINING_DATASET'])\n",
        "documents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXtHGtq_rIZl"
      },
      "source": [
        "# Read the validation dataset from .csv file\n",
        "documents_val = pd.read_csv(CONSTANTS['VALIDATION_DATASET'])\n",
        "documents_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpf9IT7xlfMK"
      },
      "source": [
        "## Definição dos conjunto de dados de acordo com o tamanho da ementa\n",
        "\n",
        "O Bert tem uma limitação de lidar apenas com textos de, no máximo, 512 tokens. Portanto, uma forma de contornar a situação seria utilizar apenas o início da ementa até atingir essa capacidade máxima que o Bert oferece.\n",
        "\n",
        "Uma imagem que explica o método que será utilizado:\n",
        "\n",
        "![long-sequences-bert](https://drive.google.com/uc?export=view&id=1I-VK8Zy_SZurOl41es8gNSRYtsGZsSfq)\n",
        "\n",
        "No caso, quando um acórdão possui uma ementa grande, apenas os 512 tokens iniciais serão utilizados.\n",
        "\n",
        "Para uma abordagem como essa, não é necessário ter porções de implementação para tratar os acórdãos maiores. O BertTokenizer irá lidar com essas ementas maiores truncando o conteúdo para reduzir o tamanho de acordo com sua limitação."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-Vbqi0hqMQM"
      },
      "source": [
        "## Preparação para o treinamento do modelo\n",
        "\n",
        "Depois de ter os dados organizados, a melhor forma de treinar o modelo é antecipar uma preparação dos dados. Dessa forma, geralmente cria-se um Dataset para que o modelo possa consumir os dados facilmente.\n",
        "\n",
        "O Dataset auxilia a modularizar o código utilizado para treinar o modelo. Dessa forma, as rotinas para manter uma coleção de dados utilizada para o modelo podem ser isoladas no Dataset. O Dataset basicamente compreende amostras de dados com seus respectivos rótulos (saída do modelo)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEZHjpXVhrCm"
      },
      "source": [
        "class LawDocumentDataset(Dataset):\n",
        "  def __init__(self, dataframe: pd.DataFrame, tokenizer: BertTokenizer, max_token_length: int=512):\n",
        "    self.dataframe = dataframe\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_token_length = max_token_length\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataframe)\n",
        "\n",
        "  def __getitem__(self, index: int):\n",
        "    row = self.dataframe.iloc[index]\n",
        "    summary_document = row.ementa\n",
        "    law_branch_id = row.ramo\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      summary_document,\n",
        "      add_special_tokens=True,          # Add `[CLS]` and `[SEP]`\n",
        "      max_length=self.max_token_length,\n",
        "      return_token_type_ids=False,\n",
        "      padding=\"max_length\",\n",
        "      truncation=True,                  # Truncate encoding to the max length\n",
        "      return_attention_mask=True,       # Return attention mask\n",
        "      return_tensors=\"pt\"               # Return PyTorch tensor\n",
        "    )\n",
        "\n",
        "    labels = np.eye(HYPERPARAMETERS['NUMBER_OF_BRANCHES'])[law_branch_id]  # Return a list with zeros, except for index law_branch_id that assumes one\n",
        "\n",
        "    return dict(\n",
        "        summary_document=summary_document,\n",
        "        input_ids=encoding[\"input_ids\"].flatten(),\n",
        "        attention_mask=encoding[\"attention_mask\"].flatten(),\n",
        "        labels=torch.FloatTensor(labels)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcGiRXchwBpx"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(CONSTANTS['BERT_MODEL_NAME'])\n",
        "tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp_vYWBHkZPj"
      },
      "source": [
        "train_dataset = LawDocumentDataset(documents, tokenizer, HYPERPARAMETERS['MAX_NUMBER_TOKENS'])\n",
        "validation_dataset = LawDocumentDataset(documents_val, tokenizer, HYPERPARAMETERS['MAX_NUMBER_TOKENS'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L71-FU5xzpYY"
      },
      "source": [
        "## Criando o modelo\n",
        "\n",
        "Depois de ter toda a preparação dos dados, o modelo pode então começar a ser treinado. Para isso, utilizar LightiningModule pode auxiliar durante o processo. O código normalmente utilizado para o treinamento de uma rede neural usando Pytorch pode ser compactado por meio do LightningModule. LightningModule permite que o treinamento do modelo esteja disposto de uma forma organizada no código, também prevenindo que chamadas utilizando `.cuda()` ou `.to()` sejam realizadas. A própria classe se responsabiliza para controlar quais tensores devem abrigar cálculos dentro da GPU.\n",
        "\n",
        "Basicamente, o processo utilizado pelo LightningModule para o treinamento do modelo é:\n",
        "\n",
        "```\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    for batch in train_loader:\n",
        "        for each entry in batch, run forward\n",
        "        run training_step\n",
        "        calculate loss & metrics\n",
        "    \n",
        "    # Validation phase\n",
        "    for batch in validation_loader:\n",
        "        run validation_step\n",
        "        calculate loss & metrics\n",
        "    \n",
        "    # Test step\n",
        "    for batch in test_loader:\n",
        "        run test_step\n",
        "        calculate loss & metrics\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkFYPqTSzr3s"
      },
      "source": [
        "class LawDocumentClassifier(pl.LightningModule):\n",
        "    \n",
        "    def __init__(self, number_classes: int, steps_per_epoch: int=None, epochs: int=None, learning_rate: float=2e-5, weight_decay: float=0.01, warm_up_proportion: float=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.model = BertForSequenceClassification.from_pretrained(\n",
        "            \"neuralmind/bert-large-portuguese-cased\",\n",
        "            num_labels=number_classes,                      # The number of output labels--2 for binary classification\n",
        "            output_attentions=False,                        # Returns attention weights\n",
        "            output_hidden_states=False                      # Returns all hidden states\n",
        "        )\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.warm_up_proportion = warm_up_proportion\n",
        "        self.weight_decay = weight_decay\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        output = self.model(input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            labels=labels,\n",
        "                            return_dict=True)\n",
        "        \n",
        "        return output.loss, output.logits\n",
        "        \n",
        "    def training_step(self, batch, batch_index):\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "        \n",
        "        loss, outputs = self(input_ids, attention_mask, labels)\n",
        "        \n",
        "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
        "        \n",
        "        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
        "\n",
        "    def compute_metrics(self, eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        \n",
        "        classification_predictions = self.convert_to_classification_labels(logits)\n",
        "        classification_labels = self.convert_to_classification_labels(labels)\n",
        "\n",
        "        metrics = {\n",
        "            \"validation_accuracy\": accuracy_score(classification_labels, classification_predictions),\n",
        "            \"validation_precision\": precision_score(classification_labels, classification_predictions, average='weighted'),\n",
        "            \"validation_recall\": recall_score(classification_labels, classification_predictions, average='weighted'),\n",
        "            \"validation_f1\": f1_score(classification_labels, classification_predictions, average='weighted'),\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "            \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = AdamW(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
        "        warmup_steps = self.steps_per_epoch * self.warm_up_proportion\n",
        "        total_steps = self.steps_per_epoch * self.epochs - warmup_steps\n",
        "\n",
        "        scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
        "        \n",
        "        return (optimizer, scheduler)\n",
        "\n",
        "    def convert_to_classification_labels(self, classifications):\n",
        "        formatted_classifications = []\n",
        "\n",
        "        for classification in classifications:\n",
        "            formatted_classifications.append(np.argmax(classification).flatten())\n",
        "\n",
        "        return formatted_classifications"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm1LBkxs3wK9"
      },
      "source": [
        "model = LawDocumentClassifier(\n",
        "    HYPERPARAMETERS['NUMBER_OF_BRANCHES'],\n",
        "    steps_per_epoch=len(documents) // HYPERPARAMETERS['BATCH_SIZE'],\n",
        "    epochs=HYPERPARAMETERS['EPOCHS'],\n",
        "    learning_rate=HYPERPARAMETERS['LEARNING_RATE'],\n",
        "    weight_decay=HYPERPARAMETERS['WEIGHT_DECAY'],\n",
        "    warm_up_proportion=HYPERPARAMETERS['WARM_UP_PROPORTION']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ekt66fT_iqy4"
      },
      "source": [
        "## Treinando o modelo\n",
        "\n",
        "Basicamente, o processo utilizado pelo LightningModule para o treinamento do modelo é:\n",
        "\n",
        "```\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    for batch in train_loader:\n",
        "        for each entry in batch, run forward\n",
        "        run training_step\n",
        "        calculate loss & metrics\n",
        "    \n",
        "    # Validation phase\n",
        "    for batch in validation_loader:\n",
        "        run validation_step\n",
        "        calculate loss & metrics\n",
        "    \n",
        "    # Test step\n",
        "    for batch in test_loader:\n",
        "        run test_step\n",
        "        calculate loss & metrics\n",
        "```\n",
        "\n",
        "O processo para treinamento do modelo será executado pela API de [Trainer](https://huggingface.co/transformers/main_classes/trainer.html) do HuggingFace. Nesse sentido, alguns parâmetros são definidos durante a criação do objeto `Trainer` para a criação da estratégia de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2Kjq8BR40Yh"
      },
      "source": [
        "training_args = TrainingArguments(\n",
        "    \"/content/drive/My Drive/MAC499 - Kaique e Yurick/Projeto/trainer_output\",\n",
        "    num_train_epochs=HYPERPARAMETERS['EPOCHS'],\n",
        "    evaluation_strategy='epoch',\n",
        "    per_device_train_batch_size=HYPERPARAMETERS['BATCH_SIZE'],\n",
        "    logging_steps=30\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    compute_metrics=model.compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    optimizers=model.configure_optimizers()\n",
        ")\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDo-EYDvjidO"
      },
      "source": [
        "## Análise de métricas\n",
        "\n",
        "Após a realização do treinamento do modelo, o ideal é extrair as métricas que foram coletadas durante os processos de treinamento e validação em cada uma das épocas, para que então os melhores valores de hiperparâmetros sejam extraídos.\n",
        "\n",
        "A API `Trainer` de HuggingFace possui a capacidade de calcular automaticamente as métricas configuradas e coletar os valores durante as épocas. Esses valores serão extraídos para análises, como parte do processo de fine tuning dos hiperparâmetros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm7mIjCH9M-x"
      },
      "source": [
        "validation_metrics = trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl280YKbDfu4"
      },
      "source": [
        "validation_metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZloXL9B6Wnv"
      },
      "source": [
        "# torch.save(model.state_dict(), '/content/drive/My Drive/MAC499 - Kaique e Yurick/Projeto/truncated-2e5-0.01-0.3.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr09wffgLtUS"
      },
      "source": [
        "- https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "- https://arxiv.org/pdf/1711.05101.pdf\n",
        "- https://arxiv.org/pdf/1905.05583.pdf\n",
        "- https://arxiv.org/pdf/1810.04805.pdf\n",
        "- https://www.fast.ai/2018/07/02/adam-weight-decay/\n",
        "- Weight decay values: https://openreview.net/pdf?id=Syx4wnEtvH\n",
        "\n",
        "Cross-entropy Loss\n",
        "Optimizer = AdamW (valores podem ser retirados [daqui](https://arxiv.org/pdf/1905.05583.pdf))\n",
        "Scheduler = Cosine, provavelmente sem reset (Appendix C do [paper](https://arxiv.org/pdf/1711.05101.pdf))"
      ]
    }
  ]
}